{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "from prep import preprocess\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gensim import models\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('/Users/shivamlalakiya/Desktop/Projects/Duplicate_questions_Quora/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.key_to_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process texts in datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = []\n",
    "text_2 = []\n",
    "labels = []\n",
    "\n",
    "p = preprocess()\n",
    "\n",
    "with codecs.open('/Users/shivamlalakiya/Desktop/Projects/Duplicate_questions_Quora/train.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        text_1.append(p.text_to_worldlist(values[3]))\n",
    "        text_2.append(p.text_to_worldlist(values[4]))\n",
    "        labels.append(int(values[5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_1 + text_2)\n",
    "\n",
    "with open('/Users/shivamlalakiya/Desktop/Projects/Duplicate_questions_Quora/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(text_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/xxfyhrd974n7y618tr2tkg0h0000gn/T/ipykernel_47251/1934822239.py:6: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embedding_matrix[i] = word2vec.word_vec(word)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 37391\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1,activation= 'sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 30, 300)      25655700    ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 181)          348968      ['embedding_1[0][0]',            \n",
      "                                                                  'embedding_1[1][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 362)          0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm_1[1][0]']                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 362)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 362)         1448        ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 115)          41745       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 115)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 115)         460         ['dropout_3[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            116         ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,048,437\n",
      "Trainable params: 391,783\n",
      "Non-trainable params: 25,656,654\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.4551 - acc: 0.6699 - f1_m: 0.3498 - precision_m: 0.7235 - recall_m: 0.2604WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 457s 1s/step - loss: 0.4551 - acc: 0.6699 - f1_m: 0.3498 - precision_m: 0.7235 - recall_m: 0.2604 - val_loss: 0.3838 - val_acc: 0.6652 - val_f1_m: 0.1992 - val_precision_m: 0.9016 - val_recall_m: 0.1120\n",
      "Epoch 2/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3717 - acc: 0.6942 - f1_m: 0.3383 - precision_m: 0.8373 - recall_m: 0.2130WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 405s 1s/step - loss: 0.3717 - acc: 0.6942 - f1_m: 0.3383 - precision_m: 0.8373 - recall_m: 0.2130 - val_loss: 0.3485 - val_acc: 0.7026 - val_f1_m: 0.3678 - val_precision_m: 0.8766 - val_recall_m: 0.2329\n",
      "Epoch 3/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3547 - acc: 0.7090 - f1_m: 0.3938 - precision_m: 0.8503 - recall_m: 0.2570WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 435s 1s/step - loss: 0.3547 - acc: 0.7090 - f1_m: 0.3938 - precision_m: 0.8503 - recall_m: 0.2570 - val_loss: 0.3465 - val_acc: 0.7377 - val_f1_m: 0.5117 - val_precision_m: 0.8306 - val_recall_m: 0.3700\n",
      "Epoch 4/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3425 - acc: 0.7203 - f1_m: 0.4332 - precision_m: 0.8580 - recall_m: 0.2903WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 438s 1s/step - loss: 0.3425 - acc: 0.7203 - f1_m: 0.4332 - precision_m: 0.8580 - recall_m: 0.2903 - val_loss: 0.3349 - val_acc: 0.7433 - val_f1_m: 0.5221 - val_precision_m: 0.8473 - val_recall_m: 0.3775\n",
      "Epoch 5/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3328 - acc: 0.7289 - f1_m: 0.4617 - precision_m: 0.8627 - recall_m: 0.3156WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 427s 1s/step - loss: 0.3328 - acc: 0.7289 - f1_m: 0.4617 - precision_m: 0.8627 - recall_m: 0.3156 - val_loss: 0.3206 - val_acc: 0.7517 - val_f1_m: 0.5471 - val_precision_m: 0.8485 - val_recall_m: 0.4040\n",
      "Epoch 6/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3245 - acc: 0.7368 - f1_m: 0.4859 - precision_m: 0.8690 - recall_m: 0.3376WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 412s 1s/step - loss: 0.3245 - acc: 0.7368 - f1_m: 0.4859 - precision_m: 0.8690 - recall_m: 0.3376 - val_loss: 0.3151 - val_acc: 0.7511 - val_f1_m: 0.5366 - val_precision_m: 0.8698 - val_recall_m: 0.3882\n",
      "Epoch 7/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3178 - acc: 0.7432 - f1_m: 0.5052 - precision_m: 0.8735 - recall_m: 0.3557WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 409s 1s/step - loss: 0.3178 - acc: 0.7432 - f1_m: 0.5052 - precision_m: 0.8735 - recall_m: 0.3557 - val_loss: 0.3123 - val_acc: 0.7699 - val_f1_m: 0.5994 - val_precision_m: 0.8489 - val_recall_m: 0.4634\n",
      "Epoch 8/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3124 - acc: 0.7484 - f1_m: 0.5207 - precision_m: 0.8765 - recall_m: 0.3706WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 482s 1s/step - loss: 0.3124 - acc: 0.7484 - f1_m: 0.5207 - precision_m: 0.8765 - recall_m: 0.3706 - val_loss: 0.3060 - val_acc: 0.7621 - val_f1_m: 0.5690 - val_precision_m: 0.8696 - val_recall_m: 0.4230\n",
      "Epoch 9/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3073 - acc: 0.7539 - f1_m: 0.5364 - precision_m: 0.8794 - recall_m: 0.3862WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 473s 1s/step - loss: 0.3073 - acc: 0.7539 - f1_m: 0.5364 - precision_m: 0.8794 - recall_m: 0.3862 - val_loss: 0.3009 - val_acc: 0.7784 - val_f1_m: 0.6196 - val_precision_m: 0.8550 - val_recall_m: 0.4860\n",
      "Epoch 10/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3027 - acc: 0.7589 - f1_m: 0.5506 - precision_m: 0.8817 - recall_m: 0.4006WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 463s 1s/step - loss: 0.3027 - acc: 0.7589 - f1_m: 0.5506 - precision_m: 0.8817 - recall_m: 0.4006 - val_loss: 0.3072 - val_acc: 0.7820 - val_f1_m: 0.6301 - val_precision_m: 0.8525 - val_recall_m: 0.4999\n",
      "Epoch 11/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2982 - acc: 0.7633 - f1_m: 0.5622 - precision_m: 0.8843 - recall_m: 0.4124WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 412s 1s/step - loss: 0.2982 - acc: 0.7633 - f1_m: 0.5622 - precision_m: 0.8843 - recall_m: 0.4124 - val_loss: 0.2936 - val_acc: 0.7686 - val_f1_m: 0.5817 - val_precision_m: 0.8851 - val_recall_m: 0.4334\n",
      "Epoch 12/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2949 - acc: 0.7660 - f1_m: 0.5692 - precision_m: 0.8868 - recall_m: 0.4195WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 474s 1s/step - loss: 0.2949 - acc: 0.7660 - f1_m: 0.5692 - precision_m: 0.8868 - recall_m: 0.4195 - val_loss: 0.2945 - val_acc: 0.7777 - val_f1_m: 0.6127 - val_precision_m: 0.8676 - val_recall_m: 0.4738\n",
      "Epoch 13/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2914 - acc: 0.7693 - f1_m: 0.5783 - precision_m: 0.8880 - recall_m: 0.4291WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 460s 1s/step - loss: 0.2914 - acc: 0.7693 - f1_m: 0.5783 - precision_m: 0.8880 - recall_m: 0.4291 - val_loss: 0.2914 - val_acc: 0.7858 - val_f1_m: 0.6343 - val_precision_m: 0.8671 - val_recall_m: 0.5003\n",
      "Epoch 14/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2884 - acc: 0.7727 - f1_m: 0.5870 - precision_m: 0.8900 - recall_m: 0.4382WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 475s 1s/step - loss: 0.2884 - acc: 0.7727 - f1_m: 0.5870 - precision_m: 0.8900 - recall_m: 0.4382 - val_loss: 0.3000 - val_acc: 0.7960 - val_f1_m: 0.6689 - val_precision_m: 0.8420 - val_recall_m: 0.5550\n",
      "Epoch 15/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2858 - acc: 0.7751 - f1_m: 0.5934 - precision_m: 0.8913 - recall_m: 0.4450WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 438s 1s/step - loss: 0.2858 - acc: 0.7751 - f1_m: 0.5934 - precision_m: 0.8913 - recall_m: 0.4450 - val_loss: 0.2887 - val_acc: 0.7893 - val_f1_m: 0.6399 - val_precision_m: 0.8748 - val_recall_m: 0.5046\n",
      "Epoch 16/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2827 - acc: 0.7782 - f1_m: 0.6012 - precision_m: 0.8930 - recall_m: 0.4535WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 479s 1s/step - loss: 0.2827 - acc: 0.7782 - f1_m: 0.6012 - precision_m: 0.8930 - recall_m: 0.4535 - val_loss: 0.2842 - val_acc: 0.7906 - val_f1_m: 0.6448 - val_precision_m: 0.8720 - val_recall_m: 0.5117\n",
      "Epoch 17/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2802 - acc: 0.7806 - f1_m: 0.6071 - precision_m: 0.8950 - recall_m: 0.4596WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 421s 1s/step - loss: 0.2802 - acc: 0.7806 - f1_m: 0.6071 - precision_m: 0.8950 - recall_m: 0.4596 - val_loss: 0.2834 - val_acc: 0.7900 - val_f1_m: 0.6410 - val_precision_m: 0.8775 - val_recall_m: 0.5051\n",
      "Epoch 18/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2779 - acc: 0.7834 - f1_m: 0.6135 - precision_m: 0.8971 - recall_m: 0.4664WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 409s 1s/step - loss: 0.2779 - acc: 0.7834 - f1_m: 0.6135 - precision_m: 0.8971 - recall_m: 0.4664 - val_loss: 0.2926 - val_acc: 0.8055 - val_f1_m: 0.6889 - val_precision_m: 0.8480 - val_recall_m: 0.5802\n",
      "Epoch 19/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2759 - acc: 0.7852 - f1_m: 0.6182 - precision_m: 0.8975 - recall_m: 0.4718WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 390s 1s/step - loss: 0.2759 - acc: 0.7852 - f1_m: 0.6182 - precision_m: 0.8975 - recall_m: 0.4718 - val_loss: 0.2786 - val_acc: 0.7856 - val_f1_m: 0.6252 - val_precision_m: 0.8907 - val_recall_m: 0.4819\n",
      "Epoch 20/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2739 - acc: 0.7869 - f1_m: 0.6224 - precision_m: 0.8983 - recall_m: 0.4766WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 407s 1s/step - loss: 0.2739 - acc: 0.7869 - f1_m: 0.6224 - precision_m: 0.8983 - recall_m: 0.4766 - val_loss: 0.2861 - val_acc: 0.8076 - val_f1_m: 0.6922 - val_precision_m: 0.8522 - val_recall_m: 0.5831\n"
     ]
    }
   ],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input],  outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.summary()\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=20, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "bst_val_score = min(hist.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: LSTM_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('LSTM_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding the Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "50/50 [==============================] - 85s 2s/step\n",
      "50/50 [==============================] - 82s 2s/step\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/Users/shivamlalakiya/Desktop/Projects/Duplicate_questions_Quora/LSTM_model', compile=False)\n",
    "pred = model.predict([data_1, data_2], batch_size = 8192, verbose = 1)\n",
    "pred += model.predict([data_2, data_2], batch_size = 8192, verbose = 1)\n",
    "pred /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.285184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.277689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.138951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.242688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404285</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404286</th>\n",
       "      <td>1</td>\n",
       "      <td>0.678957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404287</th>\n",
       "      <td>0</td>\n",
       "      <td>0.286156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404288</th>\n",
       "      <td>0</td>\n",
       "      <td>0.337355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404289</th>\n",
       "      <td>0</td>\n",
       "      <td>0.341520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404290 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  prediction\n",
       "0           0    0.306899\n",
       "1           0    0.285184\n",
       "2           0    0.277689\n",
       "3           0    0.138951\n",
       "4           0    0.242688\n",
       "...       ...         ...\n",
       "404285      0    0.015776\n",
       "404286      1    0.678957\n",
       "404287      0    0.286156\n",
       "404288      0    0.337355\n",
       "404289      0    0.341520\n",
       "\n",
       "[404290 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/shivamlalakiya/Desktop/Projects/Duplicate_questions_Quora/train.csv')\n",
    "final = pd.DataFrame({'label':df['is_duplicate'], 'prediction':pred.ravel()})\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.5545403837572513 0.40739073437384055\n",
      "0.2 0.6339220717867228 0.5783299116970492\n",
      "0.30000000000000004 0.7537671763326179 0.7798016275445844\n",
      "0.4 0.7583451394753187 0.8287071161789804\n",
      "0.5 0.6960104553023609 0.8153182121744292\n",
      "0.6 0.6115327827771332 0.7871948353904376\n",
      "0.7 0.5219372662151921 0.7581661678498108\n",
      "0.7999999999999999 0.428447964298622 0.7300972074500977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "thr = 0\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "threshold = []\n",
    "for i in range(10):\n",
    "    thr += 0.1\n",
    "    pl = []\n",
    "    for row in range(len(final)):\n",
    "        if final['prediction'].iloc[row] > thr:\n",
    "            pl.append(1)\n",
    "        else:\n",
    "            pl.append(0)\n",
    "\n",
    "    acc = accuracy_score(final['label'], pl)\n",
    "    f1 = f1_score(final['label'], pl)\n",
    "\n",
    "    print(thr,f1,acc)\n",
    "    f1_list.append(f1)\n",
    "    acc_list.append(acc)\n",
    "    threshold.append(thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(threshold, f1_list, label = \"F1 Score\")\n",
    "plt.plot(threshold, acc_list, label = \"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duplicate_question",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
