{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from tensorflow.keras import activations\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gensim import models\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('/Users/shivamlalakiya/Desktop/Projects/Duplicate_questions_Quora/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.key_to_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process texts in datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404290 texts in train.csv\n"
     ]
    }
   ],
   "source": [
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open('/Users/shivamlalakiya/Desktop/Projects/Duplicate_questions_Quora/train.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85518 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 )\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/xxfyhrd974n7y618tr2tkg0h0000gn/T/ipykernel_47251/1934822239.py:6: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embedding_matrix[i] = word2vec.word_vec(word)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 37391\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1,activation= 'sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 30, 300)      25655700    ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 181)          348968      ['embedding_1[0][0]',            \n",
      "                                                                  'embedding_1[1][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 362)          0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm_1[1][0]']                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 362)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 362)         1448        ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 115)          41745       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 115)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 115)         460         ['dropout_3[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            116         ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,048,437\n",
      "Trainable params: 391,783\n",
      "Non-trainable params: 25,656,654\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.4551 - acc: 0.6699 - f1_m: 0.3498 - precision_m: 0.7235 - recall_m: 0.2604WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 457s 1s/step - loss: 0.4551 - acc: 0.6699 - f1_m: 0.3498 - precision_m: 0.7235 - recall_m: 0.2604 - val_loss: 0.3838 - val_acc: 0.6652 - val_f1_m: 0.1992 - val_precision_m: 0.9016 - val_recall_m: 0.1120\n",
      "Epoch 2/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3717 - acc: 0.6942 - f1_m: 0.3383 - precision_m: 0.8373 - recall_m: 0.2130WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 405s 1s/step - loss: 0.3717 - acc: 0.6942 - f1_m: 0.3383 - precision_m: 0.8373 - recall_m: 0.2130 - val_loss: 0.3485 - val_acc: 0.7026 - val_f1_m: 0.3678 - val_precision_m: 0.8766 - val_recall_m: 0.2329\n",
      "Epoch 3/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3547 - acc: 0.7090 - f1_m: 0.3938 - precision_m: 0.8503 - recall_m: 0.2570WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 435s 1s/step - loss: 0.3547 - acc: 0.7090 - f1_m: 0.3938 - precision_m: 0.8503 - recall_m: 0.2570 - val_loss: 0.3465 - val_acc: 0.7377 - val_f1_m: 0.5117 - val_precision_m: 0.8306 - val_recall_m: 0.3700\n",
      "Epoch 4/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3425 - acc: 0.7203 - f1_m: 0.4332 - precision_m: 0.8580 - recall_m: 0.2903WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 438s 1s/step - loss: 0.3425 - acc: 0.7203 - f1_m: 0.4332 - precision_m: 0.8580 - recall_m: 0.2903 - val_loss: 0.3349 - val_acc: 0.7433 - val_f1_m: 0.5221 - val_precision_m: 0.8473 - val_recall_m: 0.3775\n",
      "Epoch 5/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3328 - acc: 0.7289 - f1_m: 0.4617 - precision_m: 0.8627 - recall_m: 0.3156WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 427s 1s/step - loss: 0.3328 - acc: 0.7289 - f1_m: 0.4617 - precision_m: 0.8627 - recall_m: 0.3156 - val_loss: 0.3206 - val_acc: 0.7517 - val_f1_m: 0.5471 - val_precision_m: 0.8485 - val_recall_m: 0.4040\n",
      "Epoch 6/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3245 - acc: 0.7368 - f1_m: 0.4859 - precision_m: 0.8690 - recall_m: 0.3376WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 412s 1s/step - loss: 0.3245 - acc: 0.7368 - f1_m: 0.4859 - precision_m: 0.8690 - recall_m: 0.3376 - val_loss: 0.3151 - val_acc: 0.7511 - val_f1_m: 0.5366 - val_precision_m: 0.8698 - val_recall_m: 0.3882\n",
      "Epoch 7/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3178 - acc: 0.7432 - f1_m: 0.5052 - precision_m: 0.8735 - recall_m: 0.3557WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 409s 1s/step - loss: 0.3178 - acc: 0.7432 - f1_m: 0.5052 - precision_m: 0.8735 - recall_m: 0.3557 - val_loss: 0.3123 - val_acc: 0.7699 - val_f1_m: 0.5994 - val_precision_m: 0.8489 - val_recall_m: 0.4634\n",
      "Epoch 8/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3124 - acc: 0.7484 - f1_m: 0.5207 - precision_m: 0.8765 - recall_m: 0.3706WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 482s 1s/step - loss: 0.3124 - acc: 0.7484 - f1_m: 0.5207 - precision_m: 0.8765 - recall_m: 0.3706 - val_loss: 0.3060 - val_acc: 0.7621 - val_f1_m: 0.5690 - val_precision_m: 0.8696 - val_recall_m: 0.4230\n",
      "Epoch 9/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3073 - acc: 0.7539 - f1_m: 0.5364 - precision_m: 0.8794 - recall_m: 0.3862WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 473s 1s/step - loss: 0.3073 - acc: 0.7539 - f1_m: 0.5364 - precision_m: 0.8794 - recall_m: 0.3862 - val_loss: 0.3009 - val_acc: 0.7784 - val_f1_m: 0.6196 - val_precision_m: 0.8550 - val_recall_m: 0.4860\n",
      "Epoch 10/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3027 - acc: 0.7589 - f1_m: 0.5506 - precision_m: 0.8817 - recall_m: 0.4006WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 463s 1s/step - loss: 0.3027 - acc: 0.7589 - f1_m: 0.5506 - precision_m: 0.8817 - recall_m: 0.4006 - val_loss: 0.3072 - val_acc: 0.7820 - val_f1_m: 0.6301 - val_precision_m: 0.8525 - val_recall_m: 0.4999\n",
      "Epoch 11/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2982 - acc: 0.7633 - f1_m: 0.5622 - precision_m: 0.8843 - recall_m: 0.4124WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 412s 1s/step - loss: 0.2982 - acc: 0.7633 - f1_m: 0.5622 - precision_m: 0.8843 - recall_m: 0.4124 - val_loss: 0.2936 - val_acc: 0.7686 - val_f1_m: 0.5817 - val_precision_m: 0.8851 - val_recall_m: 0.4334\n",
      "Epoch 12/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2949 - acc: 0.7660 - f1_m: 0.5692 - precision_m: 0.8868 - recall_m: 0.4195WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 474s 1s/step - loss: 0.2949 - acc: 0.7660 - f1_m: 0.5692 - precision_m: 0.8868 - recall_m: 0.4195 - val_loss: 0.2945 - val_acc: 0.7777 - val_f1_m: 0.6127 - val_precision_m: 0.8676 - val_recall_m: 0.4738\n",
      "Epoch 13/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2914 - acc: 0.7693 - f1_m: 0.5783 - precision_m: 0.8880 - recall_m: 0.4291WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 460s 1s/step - loss: 0.2914 - acc: 0.7693 - f1_m: 0.5783 - precision_m: 0.8880 - recall_m: 0.4291 - val_loss: 0.2914 - val_acc: 0.7858 - val_f1_m: 0.6343 - val_precision_m: 0.8671 - val_recall_m: 0.5003\n",
      "Epoch 14/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2884 - acc: 0.7727 - f1_m: 0.5870 - precision_m: 0.8900 - recall_m: 0.4382WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 475s 1s/step - loss: 0.2884 - acc: 0.7727 - f1_m: 0.5870 - precision_m: 0.8900 - recall_m: 0.4382 - val_loss: 0.3000 - val_acc: 0.7960 - val_f1_m: 0.6689 - val_precision_m: 0.8420 - val_recall_m: 0.5550\n",
      "Epoch 15/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2858 - acc: 0.7751 - f1_m: 0.5934 - precision_m: 0.8913 - recall_m: 0.4450WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 438s 1s/step - loss: 0.2858 - acc: 0.7751 - f1_m: 0.5934 - precision_m: 0.8913 - recall_m: 0.4450 - val_loss: 0.2887 - val_acc: 0.7893 - val_f1_m: 0.6399 - val_precision_m: 0.8748 - val_recall_m: 0.5046\n",
      "Epoch 16/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2827 - acc: 0.7782 - f1_m: 0.6012 - precision_m: 0.8930 - recall_m: 0.4535WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 479s 1s/step - loss: 0.2827 - acc: 0.7782 - f1_m: 0.6012 - precision_m: 0.8930 - recall_m: 0.4535 - val_loss: 0.2842 - val_acc: 0.7906 - val_f1_m: 0.6448 - val_precision_m: 0.8720 - val_recall_m: 0.5117\n",
      "Epoch 17/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2802 - acc: 0.7806 - f1_m: 0.6071 - precision_m: 0.8950 - recall_m: 0.4596WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 421s 1s/step - loss: 0.2802 - acc: 0.7806 - f1_m: 0.6071 - precision_m: 0.8950 - recall_m: 0.4596 - val_loss: 0.2834 - val_acc: 0.7900 - val_f1_m: 0.6410 - val_precision_m: 0.8775 - val_recall_m: 0.5051\n",
      "Epoch 18/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2779 - acc: 0.7834 - f1_m: 0.6135 - precision_m: 0.8971 - recall_m: 0.4664WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 409s 1s/step - loss: 0.2779 - acc: 0.7834 - f1_m: 0.6135 - precision_m: 0.8971 - recall_m: 0.4664 - val_loss: 0.2926 - val_acc: 0.8055 - val_f1_m: 0.6889 - val_precision_m: 0.8480 - val_recall_m: 0.5802\n",
      "Epoch 19/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2759 - acc: 0.7852 - f1_m: 0.6182 - precision_m: 0.8975 - recall_m: 0.4718WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 390s 1s/step - loss: 0.2759 - acc: 0.7852 - f1_m: 0.6182 - precision_m: 0.8975 - recall_m: 0.4718 - val_loss: 0.2786 - val_acc: 0.7856 - val_f1_m: 0.6252 - val_precision_m: 0.8907 - val_recall_m: 0.4819\n",
      "Epoch 20/20\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2739 - acc: 0.7869 - f1_m: 0.6224 - precision_m: 0.8983 - recall_m: 0.4766WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 407s 1s/step - loss: 0.2739 - acc: 0.7869 - f1_m: 0.6224 - precision_m: 0.8983 - recall_m: 0.4766 - val_loss: 0.2861 - val_acc: 0.8076 - val_f1_m: 0.6922 - val_precision_m: 0.8522 - val_recall_m: 0.5831\n"
     ]
    }
   ],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input],  outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.summary()\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = 'LSTM_Model' + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=20, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: LSTM_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('LSTM_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 30, 300)      25655700    ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 181)          348968      ['embedding_1[0][0]',            \n",
      "                                                                  'embedding_1[1][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 362)          0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm_1[1][0]']                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 362)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 362)         1448        ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 115)          41745       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 115)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 115)         460         ['dropout_3[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            116         ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,048,437\n",
      "Trainable params: 391,783\n",
      "Non-trainable params: 25,656,654\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('LSTM_model',  compile=False)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duplicate_question",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
